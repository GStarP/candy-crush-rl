# 糖豆战阵 强化学习环境设计文档

## 观察空间设计

### 设计原则

- 采用多层特征图结构，将游戏状态分解为不同语义层
- 每层特征图对应特定的游戏元素，便于模型理解和特征提取
- 利用 CNN 的空间感知能力，保持位置信息的完整性

### 观察空间结构

- **地图层**: 表示可通行区域和障碍物
- **玩家层**: 标识己方和敌方玩家位置
- **炸弹层**: 显示炸弹位置和爆炸范围
- **道具层**: 标记各类道具的分布
- **占领层**: 反映格子的占领状态

### 设计优势

- 信息分离清晰，便于模型学习不同策略
- 结构化表示符合游戏的空间特性
- 支持卷积神经网络的高效特征提取
- 易于扩展和调试

## 动作空间设计

### 设计原则

- 采用笛卡尔积展开方式，将移动和放炸弹组合为离散动作
- 平衡训练效率与策略丰富性
- 简化动作空间，提高学习效率

### 动作空间结构

- 5 个基础移动方向（上下左右静止）
- 2 个炸弹状态（放炸弹/不放炸弹）
- 组合形成 10 个离散动作选项
- 步长固定，简化控制复杂度

### 设计优势

- 动作空间简单有效，易于算法处理
- 策略覆盖完整，满足游戏需求
- 兼容主流强化学习算法
- 便于调试和分析

## 奖励函数设计

### 设计原则

- 采用三阶段渐进式设计，从简单到复杂
- 以占领格子为核心目标，避免被炸晕为关键约束
- 平衡短期行为和长期策略

### 奖励函数结构

#### 第一阶段：MVP 版本

- **占领奖励**: 成功占领新格子获得正奖励
- **被炸惩罚**: 被炸晕给予负奖励
- **胜负结果**: 游戏结束时的最终奖励

#### 第二阶段：战术扩展

- 增加击杀敌人奖励
- 添加道具收集激励
- 引入生存时间奖励

#### 第三阶段：精细化调优

- 考虑位置价值差异
- 添加团队协作奖励
- 优化时间衰减机制

### 权重设计原理

- **10:3:1 原则**: 核心目标:次要目标:辅助目标
- **损失厌恶**: 惩罚权重略高于奖励权重
- **时间尺度匹配**: 短期行为和长期目标的平衡

### 设计优势

- 渐进式学习，降低训练难度
- 核心目标聚焦，避免奖励稀疏
- 灵活调整，支持持续优化
- 符合人类直觉，易于理解和调试

## 双角色控制设计

### 设计原则

- 采用共享网络架构，两个角色使用同一个神经网络
- 通过独立动作头实现角色差异化决策
- 使用缓存机制优化推理效率
- 避免联合动作空间的复杂性

### 网络架构

- **共享特征提取**：CNN 骨干网络处理观察空间
- **独立动作头**：每个角色拥有独立的 10 维动作输出
- **角色标识**：通过 player_id 区分不同角色的请求
- **总输出维度**：20 维（10+10），而非 100 维联合空间

### 推理优化方案

- **时间戳缓存**：基于游戏状态时间戳缓存预测结果
- **一次推理**：每个时间步只进行一次前向传播
- **双输出**：同时生成两个角色的动作决策

### 接口适配

- **服务器调用**：适配两个独立的 HTTP 请求
- **状态共享**：两个请求使用相同的游戏状态
- **结果分发**：根据 player_id 返回对应角色的动作
- **缓存管理**：自动处理缓存失效和更新

### 设计优势

- **训练效率高**：两个角色共享学习经验
- **推理效率高**：避免重复计算，节省 50%推理时间
- **协作能力强**：共享底层特征，天然包含协作信息
- **调试友好**：可独立分析每个角色的决策逻辑

## 训练效率优化

- 批量学习
- 并行环境
- 经验重复利用
